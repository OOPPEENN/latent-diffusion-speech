data:
  block_size: 512
  duration: 1
  encoder: whisper_large_v3
  encoder_hop_size: 320
  encoder_sample_rate: 16000
  extensions:
    - wav
  f0_max: 1200
  f0_min: 40
  sampling_rate: 44100
  units_forced_mode: nearest
  train_path: data/train
  valid_path: data/val
############################################
common:
  n_spk: 13
  device: cuda
  vocoder:
    ckpt: pretrain/nsf_hifigan/model
    type: nsf-hifigan
  infer:
    method: unipc
    speedup: 10
############################################
diffusion:
  model:
    block_out_channels: [256,384,512,512]
    is_tts: true
    n_chans: 512
    n_heads: 8
    n_hidden: 256
    n_layers: 2
    use_pitch_aug: true
  train:
    amp_dtype: fp16
    batch_size: 50
    cache_all_data: false
    cache_device: cpu
    clip_grad_norm: 1
    decay_step: 1000000
    epochs: 100000
    expdir: exp/diffusion
    gamma: 0.5
    interval_log: 100
    interval_val: 5000
    last_save_model_num: 4
    lr: 0.00012
    num_workers: 4
    save_opt: false
    start_lr: 0.00001
    warm_up_steps: 1000
    weight_decay: 0
############################################
text2semantic:
  model:
    codebook_path: pretrain/semantic_codebook.pt
    mode: text
    semantic_kmeans_num: 2048
    type: roformer
    decoder:
      attention_probs_dropout_prob: 0.1
      hidden_act: gelu
      hidden_dropout_prob: 0.1
      hidden_size: 512
      initializer_range: 0.02
      intermediate_size: 1024
      layer_norm_eps: 1.e-12
      max_position_embeddings: 3072
      num_attention_heads: 8
      num_hidden_layers: 6
    encoder:
      attention_probs_dropout_prob: 0.1
      hidden_act: gelu
      hidden_dropout_prob: 0.1
      hidden_size: 512
      initializer_range: 0.02
      intermediate_size: 1024
      layer_norm_eps: 1.e-12
      max_position_embeddings: 3072
      num_attention_heads: 8
      num_hidden_layers: 6
  train:
    batch_size: 8
    cache_all_data: true
    clip_grad_norm: -1
    decay_step: 200000
    epochs: 100000
    expdir: exp/lm
    gamma: 0.5
    gradient_accumulation_steps: 1
    lr: 0.0002
    interval_log: 100
    interval_val: 5000
    num_workers: 2
    save_opt: true
    start_lr: 0.00001
    units_quantize_type: kmeans
    use_units_quantize: true
    warm_up_steps: 1000
    weight_decay: 0
    use_flash_attn: false